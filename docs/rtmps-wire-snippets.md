# RTMPS/FLV Wire Protocol Reference

**Purpose**: Comprehensive documentation of RTMP control messages, FLV tag construction, and encoder lifecycle integration for YouTube live streaming.

**Status**: Based on `MinimalRtmpsClient.kt` (Window ACK fix completed), `MinimalFlvMuxer.kt`, and `RootEncoderService.kt`

**RTMP Spec**: [Adobe RTMP Specification 1.0](https://www.adobe.com/devnet/rtmp.html)  
**FLV Spec**: [Adobe FLV File Format Specification v10.1](https://www.adobe.com/content/dam/acom/en/devnet/flv/video_file_format_spec_v10.pdf)

---

## Section 1: RTMP Control & Bandwidth Messages

### 1.1 Protocol Control Messages (Message Type 1-6)

**Message Types**:
- `1` = SetChunkSize (client ‚Üí server, server ‚Üí client)
- `2` = Abort (abort message stream)
- `3` = Acknowledgement (Window ACK - bytes received FROM peer)
- `5` = WindowAcknowledgementSize (peer sets ACK window)
- `6` = SetPeerBandwidth (bandwidth limit)

**Chunk Stream IDs**:
- `2` = Protocol control messages (SetChunkSize, WindowAckSize, Acknowledgement)
- `3` = Control commands (connect, createStream, etc.)
- `8` = Video data
- `9` = Audio data
- `10` = Data/metadata (@setDataFrame)

**Message Stream IDs**:
- `0` = Control messages (before createStream)
- `streamId` = Media messages (after createStream response)

---

### 1.2 SetChunkSize Implementation

**File**: `MinimalRtmpsClient.kt:520-527`

```kotlin
fun sendSetChunkSize(size: Int) {
    outChunkSize = size
    val b = ByteBuffer.allocate(4).order(ByteOrder.BIG_ENDIAN).putInt(size).array()
    sendCommand(csid = 2, messageStreamId = 0, messageType = RtmpConsts.SET_CHUNK_SIZE, streamId = 0, payload = b)
    Log.i(TAG, "‚Üí SetChunkSize=$size")
}
```

**Usage**:
```kotlin
// File: MinimalRtmpsClient.kt:403
// Increase chunk size before publish to reduce fragment overhead
sendSetChunkSize(StreamConfig.rtmpChunkSize)  // Default: 4096
```

**Protocol Details**:
- **Message Type**: `1`
- **Payload Size**: 4 bytes (32-bit big-endian integer)
- **Value Range**: 1 to 16777215 (0x00000001 to 0x00FFFFFF)
- **Default**: 128 bytes (RTMP spec)
- **Recommended**: 4096+ for HD streaming (reduces chunk header overhead)
- **Effect**: Changes maximum chunk size for subsequent messages

---

### 1.3 WindowAcknowledgementSize Implementation

**File**: `MinimalRtmpsClient.kt:514-518`

```kotlin
private fun sendWindowAckSize(size: Int) {
    val b = ByteBuffer.allocate(4).order(ByteOrder.BIG_ENDIAN).putInt(size).array()
    sendCommand(csid = 2, messageStreamId = 0, messageType = RtmpConsts.WINDOW_ACK_SIZE, streamId = 0, payload = b)
    Log.i(TAG, "‚Üí WindowAckSize=$size")
}
```

**Receiving Handler** (Server ‚Üí Client):
```kotlin
// File: MinimalRtmpsClient.kt:125-140
assembler.onWindowAck = { size -> 
    windowAckSize = size
    ackThreshold = (size * StreamConfig.ackWindowThreshold).toLong()  // 60% of windowAckSize
    bytesReadSinceLastAck = 0
    totalBytesRead = assembler.totalBytesRead
    Log.i(TAG, "‚Üê WindowAckSize=$size (will ACK at $ackThreshold bytes READ)")
}
```

**Protocol Details**:
- **Message Type**: `5`
- **Payload Size**: 4 bytes (32-bit big-endian integer)
- **Direction**: Bidirectional (both peers can set their own window)
- **Meaning**: "Acknowledge when you've received this many bytes FROM me"
- **Critical**: ACK must track bytes READ from server, not bytes SENT to server

---

### 1.4 Acknowledgement (Window ACK) - CRITICAL FIX

**File**: `MinimalRtmpsClient.kt:615-638`

```kotlin
/**
 * [PTL] CRITICAL FIX: Send Window ACK based on bytes READ from server
 * RTMP spec: "Client acknowledges receiving X bytes FROM server"
 * This is called in reader loop after each message is processed.
 */
private fun checkAndSendWindowAck() {
    if (windowAckSize <= 0) {
        return  // Server hasn't set window size yet
    }
    
    if (ackThreshold <= 0) {
        return  // Not configured
    }
    
    // Log progress every 500KB
    if (bytesReadSinceLastAck % 500_000 < 10_000) {
        PtlLog.i("RTMPS: Window ACK tracking: bytesRead=$totalBytesRead, delta=$bytesReadSinceLastAck, threshold=$ackThreshold")
    }
    
    // Send ACK when we've read enough bytes from server
    if (bytesReadSinceLastAck >= ackThreshold) {
        PtlLog.i("RTMPS: üéØ Window ACK threshold reached! Sending ACK... (bytesRead=$totalBytesRead, threshold=$ackThreshold)")
        sendAcknowledgement((totalBytesRead and 0xFFFFFFFF).toInt())  // 32-bit wrap per RTMP spec
        bytesReadSinceLastAck = 0  // Reset delta counter
    }
}

private fun sendAcknowledgement(sequenceNumber: Int) {
    val b = ByteBuffer.allocate(4).order(ByteOrder.BIG_ENDIAN).putInt(sequenceNumber).array()
    sendCommand(csid = 2, messageStreamId = 0, messageType = RtmpConsts.ACKNOWLEDGEMENT, streamId = 0, payload = b)
    PtlLog.i("RTMPS: ‚Üí Acknowledgement sequenceNumber=$sequenceNumber (0x${sequenceNumber.toString(16)})")
}
```

**Integration in Reader Loop**:
```kotlin
// File: MinimalRtmpsClient.kt:188-212
private fun startReaderThread() {
    thread(name = "rtmp-reader", isDaemon = true) {
        while (!socket.isClosed) {
            val (h, pay) = assembler.readMessage()
            
            // Track bytes READ from server (RTMP spec requirement)
            totalBytesRead = assembler.totalBytesRead
            bytesReadSinceLastAck += pay.size
            lastInboundTimestamp = System.currentTimeMillis()
            
            when (h.messageTypeId) {
                RtmpConsts.COMMAND_AMF0 -> handleCommand(pay)
                RtmpConsts.USER_CONTROL -> handleUserControl(pay)
                RtmpConsts.WINDOW_ACK_SIZE -> assembler.onWindowAck?.invoke(
                    ByteBuffer.wrap(pay).order(ByteOrder.BIG_ENDIAN).int
                )
                // ... other message types
            }
            
            // Check ACK after EVERY message
            checkAndSendWindowAck()
        }
    }
}
```

**Protocol Details**:
- **Message Type**: `3`
- **Payload Size**: 4 bytes (32-bit big-endian integer)
- **Value**: Total cumulative bytes READ from server (NOT sent to server!)
- **32-bit Wrap**: Must use `& 0xFFFFFFFF` to handle overflow after 4GB
- **Threshold**: Typically 60-90% of server's windowAckSize
- **Critical**: YouTube expects ACK on data THEY send, not on what we send
- **Bug Fixed**: Code was tracking `bytesSent` (CountingOutputStream), now tracks `bytesRead`

---

### 1.5 User Control Messages (Message Type 4)

**File**: `MinimalRtmpsClient.kt:308-360`

```kotlin
/**
 * Handle User Control Messages (message type 4)
 * Events: 0=StreamBegin, 1=StreamEof, 6=PingRequest, 7=PingResponse, etc.
 * See RTMP spec section 7.1.7 for full list.
 */
private fun handleUserControl(payload: ByteArray) {
    if (payload.size < 2) {
        Log.w(TAG, "UserControl payload too short: ${payload.size} bytes")
        return
    }
    
    val buf = ByteBuffer.wrap(payload).order(ByteOrder.BIG_ENDIAN)
    val eventType = buf.short.toInt()
    
    when (eventType) {
        0 -> {  // StreamBegin
            val streamId = buf.int
            PtlLog.i("RTMPS: ‚Üê StreamBegin streamId=$streamId")
            Log.i(TAG, "‚Üê StreamBegin streamId=$streamId")
        }
        1 -> {  // StreamEof (server closing stream)
            val streamId = buf.int
            PtlLog.i("RTMPS: ‚Üê StreamEof streamId=$streamId (server closing stream)")
            Log.w(TAG, "‚Üê StreamEof streamId=$streamId")
        }
        6 -> {  // PingRequest (server ‚Üí client)
            val timestamp = buf.int
            PtlLog.i("RTMPS: ‚Üê PingRequest timestamp=$timestamp")
            Log.i(TAG, "‚Üê PingRequest timestamp=$timestamp")
            sendPingResponse(timestamp)
        }
        7 -> {  // PingResponse (server ‚Üí client, echo of our PingRequest)
            val timestamp = buf.int
            PtlLog.i("RTMPS: ‚Üê PingResponse timestamp=$timestamp")
            Log.i(TAG, "‚Üê PingResponse timestamp=$timestamp")
        }
        else -> {
            PtlLog.d("RTMPS: ‚Üê UserControl eventType=$eventType (unhandled)")
            Log.d(TAG, "‚Üê UserControl eventType=$eventType")
        }
    }
}

private fun sendPingResponse(timestamp: Int) {
    val payload = ByteBuffer.allocate(6).order(ByteOrder.BIG_ENDIAN)
        .putShort(7)  // Event type 7 = PingResponse
        .putInt(timestamp)  // Echo the timestamp
        .array()
    sendCommand(csid = 2, messageStreamId = 0, messageType = RtmpConsts.USER_CONTROL, streamId = 0, payload = payload)
    PtlLog.i("RTMPS: ‚Üí PingResponse timestamp=$timestamp")
    Log.i(TAG, "‚Üí PingResponse timestamp=$timestamp")
}

// TODO: Implement proactive PingRequest (keep-alive)
private fun sendPingRequest(timestamp: Int) {
    val payload = ByteBuffer.allocate(6).order(ByteOrder.BIG_ENDIAN)
        .putShort(6)  // Event type 6 = PingRequest
        .putInt(timestamp)
        .array()
    sendCommand(csid = 2, messageStreamId = 0, messageType = RtmpConsts.USER_CONTROL, streamId = 0, payload = payload)
    PtlLog.i("RTMPS: ‚Üí PingRequest timestamp=$timestamp (keep-alive)")
    Log.i(TAG, "‚Üí PingRequest timestamp=$timestamp (keep-alive)")
}
```

**User Control Event Types**:
- `0` = StreamBegin (server confirms stream started)
- `1` = StreamEof (server ending stream)
- `2` = StreamDry (no more data available)
- `3` = SetBufferLength (client buffer size)
- `4` = StreamIsRecorded (stream is being recorded)
- `6` = PingRequest (must reply with event 7)
- `7` = PingResponse (echo timestamp)

**Keep-Alive Implementation** (TODO):
```kotlin
// File: MinimalRtmpsClient.kt (NOT YET IMPLEMENTED)
// Proactive PingRequest to prevent NAT/carrier timeout

private fun startKeepAliveTimer() {
    val keepAliveInterval = 10_000L  // 10 seconds
    val idleThreshold = 8_000L        // 8 seconds of no inbound traffic
    
    scope.launch {
        while (isSocketActive()) {
            delay(keepAliveInterval)
            
            val idleTime = System.currentTimeMillis() - lastInboundTimestamp
            if (idleTime > idleThreshold) {
                val timestamp = (System.currentTimeMillis() and 0xFFFFFFFF).toInt()
                sendPingRequest(timestamp)
                PtlLog.i("RTMPS: client PingRequest ts=$timestamp (keep-alive, idle for ${idleTime}ms)")
            }
        }
    }
}
```

**Protocol Details**:
- **Message Type**: `4`
- **Payload Format**: `[EventType:2 bytes][EventData:variable]`
- **EventData**: Typically 4 bytes (int), varies by event type
- **Critical**: Must respond to PingRequest (event 6) with PingResponse (event 7)
- **Keep-Alive**: Client should send proactive PingRequest every 10-15s if idle

---

### 1.6 @setDataFrame (onMetaData) Implementation

**File**: `MinimalRtmpsClient.kt:438-461`

```kotlin
// [PTL] Send @setDataFrame("onMetaData") - required by YouTube
fun sendMetadata(width: Int, height: Int, fps: Int, videoBitrate: Int, audioBitrate: Int) {
    val metadata = mapOf(
        "width" to width.toDouble(),
        "height" to height.toDouble(),
        "framerate" to fps.toDouble(),
        "videodatarate" to (videoBitrate / 1000.0),  // YouTube expects kbps
        "videocodecid" to 7.0,  // AVC/H.264
        "audiodatarate" to (audioBitrate / 1000.0),  // kbps
        "audiosamplerate" to 48000.0,
        "audiosamplesize" to 16.0,
        "audiochannels" to 1.0,
        "audiocodecid" to 10.0,  // AAC
        "encoder" to "ScreenLive RTMP Client"
    )
    
    val payload = Amf0Writer().apply {
        writeString("@setDataFrame")
        writeString("onMetaData")
        writeObject(metadata)
    }.toByteArray()
    
    sendCommand(streamCommandCsid, streamId, RtmpConsts.DATA_MESSAGE_AMF0, streamId, payload)
    PtlLog.i("RTMPS: send @setDataFrame(onMetaData) - ${width}x${height}@${fps}fps")
    Log.i(TAG, "‚Üí @setDataFrame(onMetaData) - ${width}x${height}@${fps}fps, vbr=${videoBitrate/1000}kbps, abr=${audioBitrate/1000}kbps")
}
```

**Protocol Details**:
- **Message Type**: `18` (DATA_MESSAGE_AMF0)
- **Chunk Stream ID**: `10` (data channel, distinct from video/audio)
- **Message Stream ID**: `streamId` (from createStream response)
- **Format**: AMF0 encoded: `["@setDataFrame", "onMetaData", {...}]`
- **Required Properties**:
  - `width`, `height`, `framerate` (stream dimensions)
  - `videodatarate`, `videocodecid` (7 = H.264)
  - `audiodatarate`, `audiosamplerate`, `audiosamplesize`, `audiochannels`, `audiocodecid` (10 = AAC)
- **YouTube Requirement**: Must be sent AFTER publish, BEFORE first video/audio frame
- **Typical Values**:
  - 720p60: width=1280, height=720, framerate=60, videodatarate=6000 (6 Mbps)
  - 1080p60: width=1920, height=1080, framerate=60, videodatarate=9000 (9 Mbps)

---

## Section 2: FLV Tag Writers (Video/Audio)

### 2.1 FLV Tag Structure

**FLV Tag Header** (11 bytes):
```
[TagType:1][DataSize:3][Timestamp:3][TimestampExtended:1][StreamID:3]
```

**Tag Types**:
- `8` = Audio
- `9` = Video
- `18` = Script Data (metadata)

**Extended Timestamp**:
- When timestamp >= 16777215 (0xFFFFFF), use extended timestamp
- Put lower 24 bits in Timestamp field
- Put upper 8 bits in TimestampExtended field
- 32-bit max: 0xFFFFFFFF = 49.7 days

**For RTMP Streaming**:
- FLV tags are NOT wrapped in tag headers when sent via RTMP
- Only payload data is sent (audio/video format headers + data)
- RTMP chunk header provides timestamp and message type

---

### 2.2 Video Tag Format (H.264/AVC)

**File**: `MinimalFlvMuxer.kt:80-101`

```kotlin
/**
 * PTL FIX: Create video config tag (SPS/PPS)
 * AVCPacketType = 0x00 (AVCDecoderConfigurationRecord)
 * 
 * IMPORTANT: For RTMP streaming, return ONLY the payload data
 * (no FLV tag header, no PreviousTagSize)
 */
fun createVideoConfigTag(configData: ByteArray, timestamp: Int): ByteArray {
    val frameType = 0x10  // Keyframe (bits 7-4: 1=keyframe)
    val codecId = 0x07    // AVC/H.264 (bits 3-0: 7=AVC)
    val header = byteArrayOf(
        (frameType or codecId).toByte(),
        0x00,  // AVCPacketType = 0 (AVCDecoderConfigurationRecord)
        0x00, 0x00, 0x00  // CompositionTime = 0 (24-bit, big-endian)
    )
    
    // For RTMP: return payload only (no FLV tag wrapper)
    return header + configData
}

fun createVideoTag(data: ByteArray, timestamp: Int, isKeyframe: Boolean): ByteArray {
    // AVC NALU format for RTMP:
    // [frame type + codec ID] [AVC packet type] [composition time] [data]
    
    val frameType = if (isKeyframe) 0x10 else 0x20  // 1=key, 2=inter (bits 7-4)
    val codecId = 0x07  // AVC (bits 3-0)
    val header = byteArrayOf(
        (frameType or codecId).toByte(),
        0x01,  // AVCPacketType = 1 (AVC NALU)
        0x00, 0x00, 0x00  // CompositionTime = 0 (PTS - DTS)
    )
    
    // For RTMP: return payload only
    return header + data
}
```

**Video Format Byte** (first byte):
```
Bits 7-4: FrameType
  1 = Keyframe (seekable frame)
  2 = Inter frame (P-frame)
  3 = Disposable inter frame (B-frame)
  
Bits 3-0: CodecID
  7 = AVC (H.264)
```

**AVC Packet Types**:
- `0` = AVCDecoderConfigurationRecord (SPS/PPS) - sent ONCE before first keyframe
- `1` = AVC NALU (actual video data)
- `2` = AVC end of sequence

**AVCDecoderConfigurationRecord** (configData):
```
[configurationVersion:1][AVCProfileIndication:1][profile_compatibility:1][AVCLevelIndication:1]
[lengthSizeMinusOne:1 (0xFF = 4-byte NAL length)]
[numOfSequenceParameterSets:1][SPS length:2][SPS data:variable]
[numOfPictureParameterSets:1][PPS length:2][PPS data:variable]
```

**CompositionTime** (PTS - DTS):
- For most encoders: `0` (no B-frames, PTS == DTS)
- With B-frames: `PTS - DTS` in milliseconds (can be negative for reordered frames)
- 24-bit signed integer (range: -8388608 to +8388607)

**Usage**:
```kotlin
// File: RootEncoderService.kt:520-540
// Send SPS/PPS config ONCE before first keyframe
if (bufferInfo.flags and MediaCodec.BUFFER_FLAG_CODEC_CONFIG != 0) {
    if (!videoConfigSent && bufferInfo.size > 0) {
        val configData = ByteArray(bufferInfo.size)
        outputBuffer.get(configData)
        
        val configTag = flvMuxer?.createVideoConfigTag(configData, 0)
        if (configTag != null) {
            rtmpsClient?.sendFlvData(VIDEO_MESSAGE, configTag)
            videoConfigSent = true
            PtlLogger.i(TAG, "‚úì Sent SPS/PPS (AVCDecoderConfigurationRecord) - ${configData.size} bytes")
        }
    }
}

// Send regular video frames
if (bufferInfo.size > 0) {
    val data = ByteArray(bufferInfo.size)
    outputBuffer.get(data)
    
    val timestamp = (System.currentTimeMillis() - startTime).toInt()
    val isKeyframe = (bufferInfo.flags and MediaCodec.BUFFER_FLAG_KEY_FRAME) != 0
    
    val flvTag = flvMuxer?.createVideoTag(data, timestamp, isKeyframe)
    if (flvTag != null) {
        rtmpsClient?.sendFlvData(VIDEO_MESSAGE, flvTag, timestamp)
        frameCount++
        
        if (frameCount == 1) {
            PtlLogger.i(TAG, "‚úì First video frame @ ${timestamp}ms")
        }
    }
}
```

**Critical Sequence**:
1. **SPS/PPS** (AVCDecoderConfigurationRecord) ‚Üí timestamp=0
2. **First Keyframe** ‚Üí timestamp=T (relative to stream start)
3. **Inter Frames** ‚Üí timestamps monotonically increasing
4. **Next Keyframe** ‚Üí at GOP interval (e.g., 2 seconds)

---

### 2.3 Audio Tag Format (AAC)

**File**: `MinimalFlvMuxer.kt:103-157`

```kotlin
/**
 * PTL FIX: Create audio config tag (AudioSpecificConfig)
 * AACPacketType = 0x00 (sequence header)
 * 
 * FLV Audio Tag Format:
 * Bits 7-4: Sound Format (10 = AAC)
 * Bits 3-2: Sound Rate (3 = 44kHz, used for 48kHz too)
 * Bit 1: Sound Size (1 = 16-bit)
 * Bit 0: Sound Type (0 = mono, 1 = stereo)
 * 
 * For RTMP: return payload only (no FLV tag wrapper)
 */
fun createAudioConfigTag(configData: ByteArray, timestamp: Int): ByteArray {
    // Format byte: [10(AAC) << 4] | [3(44k) << 2] | [1(16bit) << 1] | [0(mono)]
    // = 0xA0 | 0x0C | 0x02 | 0x00 = 0xAE for mono
    val soundFormat = (10 shl 4)  // 0xA0 - AAC
    val soundRate = (3 shl 2)     // 0x0C - 44kHz/48kHz
    val soundSize = (1 shl 1)     // 0x02 - 16-bit
    val soundType = 0             // 0x00 - mono (we're using CHANNEL_IN_MONO)
    
    val header = byteArrayOf(
        (soundFormat or soundRate or soundSize or soundType).toByte(),
        0x00  // AACPacketType = 0 (sequence header/config)
    )
    
    // For RTMP: return payload only
    return header + configData
}

fun createAudioTag(data: ByteArray, timestamp: Int): ByteArray {
    // AAC format for RTMP:
    // [sound format byte] [AAC packet type] [data]
    
    val soundFormat = (10 shl 4)  // 0xA0 - AAC
    val soundRate = (3 shl 2)     // 0x0C - 44kHz/48kHz
    val soundSize = (1 shl 1)     // 0x02 - 16-bit
    val soundType = 0             // 0x00 - mono
    
    val header = byteArrayOf(
        (soundFormat or soundRate or soundSize or soundType).toByte(),
        0x01  // AACPacketType = 1 (AAC raw data)
    )
    
    // For RTMP: return payload only
    return header + data
}
```

**Audio Format Byte** (first byte):
```
Bits 7-4: SoundFormat
  10 = AAC
  
Bits 3-2: SoundRate
  0 = 5.5 kHz
  1 = 11 kHz
  2 = 22 kHz
  3 = 44 kHz (also used for 48 kHz)
  
Bit 1: SoundSize
  0 = 8-bit samples
  1 = 16-bit samples
  
Bit 0: SoundType
  0 = Mono
  1 = Stereo
```

**AAC Packet Types**:
- `0` = AAC sequence header (AudioSpecificConfig) - sent ONCE before first audio frame
- `1` = AAC raw data (actual audio)

**AudioSpecificConfig** (configData):
```
[audioObjectType:5 bits][samplingFrequencyIndex:4 bits][channelConfiguration:4 bits]
[frameLengthFlag:1 bit][dependsOnCoreCoder:1 bit][extensionFlag:1 bit]
```

**Typical Values**:
- `0x12, 0x10` = AAC-LC, 48 kHz, mono (most common for streaming)
- `0x12, 0x08` = AAC-LC, 44.1 kHz, mono
- `0x11, 0x90` = AAC-LC, 44.1 kHz, stereo

**Usage**:
```kotlin
// File: RootEncoderService.kt:605-630
// Send AudioSpecificConfig ONCE before first audio frame
if (bufferInfo.flags and MediaCodec.BUFFER_FLAG_CODEC_CONFIG != 0) {
    if (!audioConfigSent && bufferInfo.size > 0) {
        val configData = ByteArray(bufferInfo.size)
        outputBuffer.get(configData)
        
        val configTag = flvMuxer?.createAudioConfigTag(configData, 0)
        if (configTag != null) {
            rtmpsClient?.sendFlvData(AUDIO_MESSAGE, configTag)
            audioConfigSent = true
            PtlLogger.i(TAG, "‚úì Sent AAC config (AudioSpecificConfig) - ${configData.size} bytes")
        }
    }
}

// Send regular audio frames
if (bufferInfo.size > 0) {
    val data = ByteArray(bufferInfo.size)
    outputBuffer.get(data)
    
    val timestamp = (System.currentTimeMillis() - startTime).toInt()
    val flvTag = flvMuxer?.createAudioTag(data, timestamp)
    if (flvTag != null) {
        rtmpsClient?.sendFlvData(AUDIO_MESSAGE, flvTag, timestamp)
        audioFrameCount++
        
        if (audioFrameCount == 1) {
            PtlLogger.i(TAG, "‚úì First audio frame @ ${timestamp}ms")
        }
    }
}
```

**Critical Sequence**:
1. **AudioSpecificConfig** (sequence header) ‚Üí timestamp=0
2. **First Audio Frame** ‚Üí timestamp=T (relative to stream start)
3. **Subsequent Frames** ‚Üí timestamps monotonically increasing (typically 23ms intervals for AAC)

---

### 2.4 Extended Timestamp Handling (TODO)

**Required for streams > 4.66 hours** (16777215 ms = 0xFFFFFF):

```kotlin
// TODO: Implement extended timestamp in sendCommandWithTimestamp()
// File: MinimalRtmpsClient.kt:560-585

private fun sendCommandWithTimestamp(csid: Int, messageStreamId: Int, messageType: Int, streamId: Int, payload: ByteArray, timestamp: Int) {
    val hdr = ByteArrayOutputStream2()
    val fmt = 0
    val basic = ((fmt and 0x03) shl 6) or (if (csid in 2..63) csid else 3)
    hdr.write(basic)

    // CRITICAL: Handle extended timestamp
    val ts = if (timestamp >= 0xFFFFFF) {
        // Extended timestamp case
        hdr.write(byteArrayOf(0xFF, 0xFF, 0xFF))  // Timestamp = 0xFFFFFF (marker)
        // ... message length, message type, message stream ID ...
        // Extended timestamp (32-bit) AFTER message header
        hdr.write(ByteBuffer.allocate(4).order(ByteOrder.BIG_ENDIAN).putInt(timestamp).array())
        timestamp
    } else {
        // Normal timestamp case (< 16777215)
        hdr.write(byteArrayOf(
            ((timestamp ushr 16) and 0xFF).toByte(),
            ((timestamp ushr 8) and 0xFF).toByte(),
            (timestamp and 0xFF).toByte()
        ))
        timestamp
    }
    
    // ... rest of implementation (message length, type, stream ID, payload chunking)
}
```

**Extended Timestamp Rules**:
1. If timestamp >= 0xFFFFFF (16777215):
   - Set timestamp field in chunk header to `0xFFFFFF`
   - Add 4-byte extended timestamp AFTER message header (before payload)
   - Extended timestamp is 32-bit big-endian (max: 0xFFFFFFFF = 49.7 days)
2. For Type 3 chunks (continuation), extended timestamp is also required if first chunk used it

**Current Status**: NOT IMPLEMENTED (streams limited to ~4.66 hours max)

---

### 2.5 Timestamp Monotonicity & DTS/PTS

**Critical Requirements**:
- Timestamps must be monotonically increasing (never go backward)
- DTS (Decode Timestamp) must always increase
- PTS (Presentation Timestamp) = DTS + CompositionTime
- For encoders without B-frames: PTS == DTS (CompositionTime = 0)
- For encoders with B-frames: CompositionTime can be negative (frame reordering)

**Implementation**:
```kotlin
// File: RootEncoderService.kt:510-530
private fun encodeVideoLoop() {
    val startTime = System.currentTimeMillis()
    var lastTimestamp = -1
    
    while (isStreaming) {
        val outputIndex = videoEncoder?.dequeueOutputBuffer(bufferInfo, 10_000) ?: continue
        
        if (outputIndex >= 0) {
            val timestamp = (System.currentTimeMillis() - startTime).toInt()
            
            // CRITICAL: Enforce monotonicity
            if (timestamp <= lastTimestamp) {
                timestamp = lastTimestamp + 1
                PtlLogger.w(TAG, "Video timestamp adjusted to maintain monotonicity: $lastTimestamp ‚Üí $timestamp")
            }
            lastTimestamp = timestamp
            
            // ... send video frame with timestamp ...
        }
    }
}
```

**Typical Frame Intervals**:
- **Video 60fps**: ~16.67ms per frame
- **Video 30fps**: ~33.33ms per frame
- **Audio AAC**: ~23ms per frame (1024 samples / 44100 Hz)

---

## Section 3: Post-Publish Encoder Trigger (MISSING - CRITICAL)

### 3.1 Current State: Publish Confirmation

**File**: `MinimalRtmpsClient.kt:286-300`

```kotlin
private fun handleOnStatus(args: List<Any?>) {
    val info = args.getOrNull(1) as? Map<*, *>
    val code = info?.get("code") as? String
    PtlLog.i("RTMPS: onStatus code=$code raw=${argsDebugString(args)}")
    Log.i(TAG, "‚Üê onStatus code=$code")

    when (code) {
        "NetStream.Publish.Start" -> published = true  // ‚ö†Ô∏è ONLY sets flag, doesn't start encoders!
        "NetStream.Publish.BadName" -> Log.e(TAG, "‚ùå Bad stream key: $code")
        else -> if (code?.contains("error", ignoreCase = true) == true || code?.contains("fail", ignoreCase = true) == true) {
            Log.e(TAG, "‚ùå Publish error: $code")
        }
    }
}
```

**Problem**: Setting `published = true` does NOT trigger encoder startup!

**Expected Sequence** (NOT IMPLEMENTED):
1. Receive `NetStream.Publish.Start`
2. Send `@setDataFrame(onMetaData)` ‚úÖ (DONE)
3. **Start encoder pipeline** ‚ùå (MISSING)
4. Send SPS/PPS (video config)
5. Send AudioSpecificConfig (audio config)
6. Start video encoding loop
7. Start audio encoding loop
8. First video frame ‚Üí YouTube
9. First audio frame ‚Üí YouTube

**Current Gap**: Steps 3-9 never happen ‚Üí YouTube disconnects after 5-7s due to empty stream

---

### 3.2 Required Implementation: onPublishStarted()

**Location**: `MinimalRtmpsClient.kt` (NOT YET IMPLEMENTED)

```kotlin
// TODO: Add callback to notify RootEncoderService when publish starts
var onPublishStarted: (() -> Unit)? = null

private fun handleOnStatus(args: List<Any?>) {
    val info = args.getOrNull(1) as? Map<*, *>
    val code = info?.get("code") as? String
    PtlLog.i("RTMPS: onStatus code=$code")
    Log.i(TAG, "‚Üê onStatus code=$code")

    when (code) {
        "NetStream.Publish.Start" -> {
            published = true
            PtlLog.i("[PTL] PUBLISH ACK received ‚Äì invoking onPublishStarted()")
            onPublishStarted?.invoke()  // ‚Üê NEW: Trigger encoder startup
        }
        "NetStream.Publish.BadName" -> Log.e(TAG, "‚ùå Bad stream key: $code")
        else -> if (code?.contains("error", ignoreCase = true) == true) {
            Log.e(TAG, "‚ùå Publish error: $code")
        }
    }
}
```

---

### 3.3 Encoder Startup Sequence

**Location**: `RootEncoderService.kt` (NOT YET IMPLEMENTED)

```kotlin
// TODO: Implement encoder startup in RootEncoderService

private fun setupRtmpsCallbacks() {
    rtmpsClient?.onPublishStarted = {
        scope.launch {
            startEncoderPipeline()
        }
    }
    
    rtmpsClient?.onDisconnected = {
        handleRtmpsDisconnect()
    }
}

private suspend fun startEncoderPipeline() {
    PtlLogger.i(TAG, "[PTL] PUBLISH ACK ‚Äì starting encoder pipeline")
    
    // 1. Ensure encoders are configured
    if (videoEncoder == null || audioEncoder == null) {
        PtlLogger.e(TAG, "‚ùå Encoders not initialized!")
        return
    }
    
    // 2. Start video encoding loop (will send SPS/PPS automatically)
    encodingJob = scope.launch {
        launch { encodeVideoLoop() }
        launch { encodeAudioLoop() }
    }
    
    PtlLogger.i(TAG, "‚úì Encoder loops started")
    
    // 3. Safety timer: Restart if no frames sent within 500ms
    delay(500)
    if (frameCount == 0) {
        PtlLogger.e(TAG, "[PTL] ‚ö†Ô∏è No video frames within 500ms ‚Äì restarting encoders")
        restartEncodersOnce()
    }
}

private fun restartEncodersOnce() {
    if (encoderRestartAttempted) {
        PtlLogger.e(TAG, "Encoder restart already attempted, giving up")
        return
    }
    encoderRestartAttempted = true
    
    PtlLogger.i(TAG, "Restarting encoders...")
    
    // Stop current encoders
    try { videoEncoder?.stop() } catch (_: Exception) {}
    try { audioEncoder?.stop() } catch (_: Exception) {}
    
    // Reinitialize and start
    videoEncoder = createVideoEncoder()
    audioEncoder = createAudioEncoder()
    
    scope.launch {
        delay(100)  // Brief pause
        startEncoderPipeline()
    }
}
```

---

### 3.4 Current Encoder State (Working but not triggered)

**File**: `RootEncoderService.kt:510-570`

```kotlin
// Video encoding loop EXISTS and WORKS correctly
private fun encodeVideoLoop() {
    val bufferInfo = MediaCodec.BufferInfo()
    val startTime = System.currentTimeMillis()
    var frameCount = 0
    var keyframeCount = 0
    var videoConfigSent = false
    
    PtlLogger.i(TAG, "Video encoding loop started")
    
    while (isStreaming) {
        try {
            val outputIndex = videoEncoder?.dequeueOutputBuffer(bufferInfo, 10_000) ?: continue
            
            if (outputIndex >= 0) {
                val outputBuffer = videoEncoder?.getOutputBuffer(outputIndex) ?: continue
                
                // Send SPS/PPS config ONCE
                if (bufferInfo.flags and MediaCodec.BUFFER_FLAG_CODEC_CONFIG != 0) {
                    if (!videoConfigSent && bufferInfo.size > 0) {
                        val configData = ByteArray(bufferInfo.size)
                        outputBuffer.get(configData)
                        
                        val configTag = flvMuxer?.createVideoConfigTag(configData, 0)
                        if (configTag != null) {
                            rtmpsClient?.sendFlvData(VIDEO_MESSAGE, configTag)
                            totalBytesSent += configTag.size
                            videoConfigSent = true
                            PtlLogger.i(TAG, "‚úì Sent SPS/PPS (AVCDecoderConfigurationRecord) - ${configData.size} bytes")
                        }
                    }
                } else if (bufferInfo.size > 0) {
                    // Regular video frames
                    val data = ByteArray(bufferInfo.size)
                    outputBuffer.get(data)
                    
                    val timestamp = (System.currentTimeMillis() - startTime).toInt()
                    val isKeyframe = (bufferInfo.flags and MediaCodec.BUFFER_FLAG_KEY_FRAME) != 0
                    
                    val flvTag = flvMuxer?.createVideoTag(data, timestamp, isKeyframe)
                    if (flvTag != null) {
                        rtmpsClient?.sendFlvData(VIDEO_MESSAGE, flvTag, timestamp)
                        totalBytesSent += flvTag.size
                    }
                    
                    frameCount++
                    
                    if (frameCount == 1) {
                        PtlLogger.i(TAG, "‚úì First video frame @ ${timestamp}ms")  // ‚Üê NEVER APPEARS IN LOGS!
                    }
                }
                
                videoEncoder?.releaseOutputBuffer(outputIndex, false)
            }
            
        } catch (e: Exception) {
            PtlLogger.e(TAG, "Video encoding error", e)
            break
        }
    }
}
```

**File**: `RootEncoderService.kt:575-650`

```kotlin
// Audio encoding loop EXISTS and WORKS correctly
private fun encodeAudioLoop() {
    val bufferInfo = MediaCodec.BufferInfo()
    val pcmBuffer = ByteArray(maxOf(audioBufferSize, 4096))
    var startTime = System.currentTimeMillis()
    var audioFrameCount = 0
    var audioConfigSent = false
    
    PtlLogger.i(TAG, "Audio encoding loop started (bufferSize=${pcmBuffer.size})")
    
    while (isStreaming) {
        try {
            // Read PCM from AudioRecord
            val read = audioRecord?.read(pcmBuffer, 0, pcmBuffer.size) ?: 0
            if (read > 0) {
                // Feed to encoder
                val inputIndex = audioEncoder?.dequeueInputBuffer(10_000) ?: continue
                if (inputIndex >= 0) {
                    val inputBuffer = audioEncoder?.getInputBuffer(inputIndex)
                    if (inputBuffer != null) {
                        inputBuffer.clear()
                        val bytesToWrite = minOf(read, inputBuffer.remaining())
                        inputBuffer.put(pcmBuffer, 0, bytesToWrite)
                        audioEncoder?.queueInputBuffer(inputIndex, 0, bytesToWrite, System.nanoTime() / 1000, 0)
                    }
                }
            }
            
            // Get encoded AAC
            val outputIndex = audioEncoder?.dequeueOutputBuffer(bufferInfo, 0) ?: continue
            if (outputIndex >= 0) {
                val outputBuffer = audioEncoder?.getOutputBuffer(outputIndex) ?: continue
                
                // Send AudioSpecificConfig ONCE
                if (bufferInfo.flags and MediaCodec.BUFFER_FLAG_CODEC_CONFIG != 0) {
                    if (!audioConfigSent && bufferInfo.size > 0) {
                        val configData = ByteArray(bufferInfo.size)
                        outputBuffer.get(configData)
                        
                        val configTag = flvMuxer?.createAudioConfigTag(configData, 0)
                        if (configTag != null) {
                            rtmpsClient?.sendFlvData(AUDIO_MESSAGE, configTag)
                            audioConfigSent = true
                            PtlLogger.i(TAG, "‚úì Sent AAC config (AudioSpecificConfig) - ${configData.size} bytes")
                        }
                    }
                } else if (bufferInfo.size > 0) {
                    // Regular AAC frames
                    val data = ByteArray(bufferInfo.size)
                    outputBuffer.get(data)
                    
                    val timestamp = (System.currentTimeMillis() - startTime).toInt()
                    val flvTag = flvMuxer?.createAudioTag(data, timestamp)
                    if (flvTag != null) {
                        rtmpsClient?.sendFlvData(AUDIO_MESSAGE, flvTag, timestamp)
                    }
                    
                    audioFrameCount++
                    
                    if (audioFrameCount == 1) {
                        PtlLogger.i(TAG, "‚úì First audio frame @ ${timestamp}ms")  // ‚Üê NEVER APPEARS IN LOGS!
                    }
                }
                
                audioEncoder?.releaseOutputBuffer(outputIndex, false)
            }
            
        } catch (e: Exception) {
            PtlLogger.e(TAG, "Audio encoding error", e)
            break
        }
    }
}
```

**Problem**: These loops are NEVER STARTED after `NetStream.Publish.Start`!

---

### 3.5 Current Start Flow (BROKEN)

**File**: `RootEncoderService.kt:270-320`

```kotlin
// Called when user taps "Start Stream"
fun startStreaming(rtmpUrl: String, streamKey: String, resultCode: Int, data: Intent) {
    scope.launch {
        try {
            // 1. Setup MediaProjection (screen capture)
            mediaProjection = getMediaProjectionManager().getMediaProjection(resultCode, data)
            
            // 2. Setup video encoder
            videoEncoder = createVideoEncoder()
            videoEncoder?.start()  // ‚Üê Encoder STARTED but no loop launched yet!
            
            // 3. Setup VirtualDisplay (captures screen ‚Üí encoder)
            virtualDisplay = mediaProjection?.createVirtualDisplay(...)
            
            // 4. Setup audio encoder
            audioEncoder = createAudioEncoder()
            audioEncoder?.start()  // ‚Üê Encoder STARTED but no loop launched yet!
            
            // 5. Setup AudioRecord (captures mic ‚Üí encoder)
            audioRecord = AudioRecord(...)
            audioRecord?.startRecording()
            
            // 6. Connect to RTMPS
            rtmpsClient = MinimalRtmpsClient()
            rtmpsClient?.connectAndPublish(rtmpUrl, streamKey)
            
            // ‚ö†Ô∏è MISSING: Launch encoding loops AFTER publish succeeds!
            // Currently encoders are started but loops never run
            
            isStreaming = true
            PtlLogger.i(TAG, "‚úì Streaming started")
            
        } catch (e: Exception) {
            PtlLogger.e(TAG, "Failed to start streaming", e)
        }
    }
}
```

**Root Cause**: 
1. Encoders are started BEFORE publish
2. Encoding loops are NEVER launched
3. `isStreaming = true` is set, but no coroutines are running `encodeVideoLoop()` or `encodeAudioLoop()`
4. YouTube sees: Connect ‚úÖ ‚Üí Publish ‚úÖ ‚Üí onMetaData ‚úÖ ‚Üí **NO VIDEO/AUDIO DATA** ‚ùå
5. YouTube disconnects after 5-7s (empty stream timeout)

---

### 3.6 Correct Implementation (TODO)

**File**: `RootEncoderService.kt` (NEEDS MAJOR REFACTOR)

```kotlin
// Step 1: Start encoders but DON'T start loops yet
fun startStreaming(rtmpUrl: String, streamKey: String, resultCode: Int, data: Intent) {
    scope.launch {
        try {
            // Setup MediaProjection, encoders, AudioRecord (same as before)
            setupMediaProjection(resultCode, data)
            setupVideoEncoder()
            setupAudioEncoder()
            
            // Connect to RTMPS
            rtmpsClient = MinimalRtmpsClient()
            rtmpsClient?.onPublishStarted = {
                // Step 2: Start encoding loops AFTER publish confirmed
                onPublishConfirmed()
            }
            rtmpsClient?.onDisconnected = {
                handleRtmpsDisconnect()
            }
            
            rtmpsClient?.connectAndPublish(rtmpUrl, streamKey)
            
            // ‚ö†Ô∏è DO NOT set isStreaming = true here!
            // Wait for onPublishStarted callback
            
        } catch (e: Exception) {
            PtlLogger.e(TAG, "Failed to start streaming", e)
        }
    }
}

// Step 2: Called when NetStream.Publish.Start received
private fun onPublishConfirmed() {
    PtlLogger.i(TAG, "[PTL] PUBLISH ACK received ‚Äì starting encoder pipeline")
    
    // Send metadata
    rtmpsClient?.sendMetadata(
        width = VIDEO_WIDTH,
        height = VIDEO_HEIGHT,
        fps = VIDEO_FPS,
        videoBitrate = VIDEO_BITRATE,
        audioBitrate = AUDIO_BITRATE
    )
    
    // NOW start encoding loops
    isStreaming = true
    encodingJob = scope.launch {
        launch { encodeVideoLoop() }
        launch { encodeAudioLoop() }
    }
    
    // Safety timer: Check if frames are flowing
    scope.launch {
        delay(500)
        if (frameCount == 0) {
            PtlLogger.e(TAG, "[PTL] ‚ö†Ô∏è No video frames within 500ms ‚Äì encoder issue!")
            // Optional: Attempt restart
            restartEncodersOnce()
        } else {
            PtlLogger.i(TAG, "‚úì Encoder pipeline healthy (frameCount=$frameCount)")
        }
    }
}
```

**Expected Log Sequence** (after fix):
```
20:00:00.123  I  RTMPS: ‚Üí publish(streamId=1, key=***abcd, type=live)
20:00:00.456  I  RTMPS: ‚Üê onStatus code=NetStream.Publish.Start
20:00:00.457  I  [PTL] PUBLISH ACK received ‚Äì starting encoder pipeline
20:00:00.458  I  RTMPS: ‚Üí @setDataFrame(onMetaData) - 1280x720@60fps
20:00:00.460  I  Video encoding loop started
20:00:00.461  I  Audio encoding loop started
20:00:00.500  I  ‚úì Sent SPS/PPS (AVCDecoderConfigurationRecord) - 42 bytes
20:00:00.520  I  ‚úì First video frame @ 60ms
20:00:00.545  I  ‚úì Sent AAC config (AudioSpecificConfig) - 2 bytes
20:00:00.568  I  ‚úì First audio frame @ 108ms
20:00:00.600  I  ‚Üê StreamBegin streamId=1
20:00:01.500  I  Window ACK: bytesRead=500000, delta=500000, threshold=1500000
20:00:02.100  I  üéØ Window ACK threshold reached! bytesRead=1500123
20:00:02.101  I  RTMPS: ‚Üí Acknowledgement sequenceNumber=1500123
... (continues streaming)
```

---

## Section 4: Integration Summary

### 4.1 Complete Message Flow

**Startup Sequence** (Client ‚Üí Server):
1. TCP connect ‚Üí YouTube RTMPS endpoint (rtmps://a.rtmp.youtube.com:443/rtmp2)
2. C0/C1 handshake (RTMP 1536-byte handshake)
3. C2 handshake complete
4. `‚Üí connect` (app = "rtmp2", tcUrl = "rtmps://...")
5. `‚Üê _result` (connect success, streamId = 0)
6. `‚Üí releaseStream` (txn = 2)
7. `‚Üí FCPublish` (txn = 3)
8. `‚Üí createStream` (txn = 4)
9. `‚Üê _result` (createStream, streamId = 1)
10. `‚Üí SetChunkSize` (4096)
11. `‚Üí publish` (streamKey, streamId = 1)
12. `‚Üê onStatus` (NetStream.Publish.Start)
13. **[MISSING] Start encoder pipeline**
14. `‚Üí @setDataFrame(onMetaData)`
15. `‚Üí Video config` (SPS/PPS)
16. `‚Üí Audio config` (AudioSpecificConfig)
17. `‚Üí Video frame 1` (keyframe)
18. `‚Üí Audio frame 1`
19. `‚Üê StreamBegin` (streamId = 1)
20. `‚Üê WindowAckSize` (2500000)
21. Continue video/audio frames...
22. `‚Üê PingRequest` (timestamp)
23. `‚Üí PingResponse` (echo timestamp)
24. `‚Üí Acknowledgement` (every 1.5MB bytesRead)
25. ... (stream continues)

**Current State**:
- Steps 1-12: ‚úÖ WORKING
- Step 13: ‚ùå MISSING (encoder loops never start)
- Steps 14-25: ‚ùå NEVER REACHED (YouTube disconnects at step 12 + 5-7s)

---

### 4.2 Critical Gaps Identified

**CRITICAL (Blocking all streaming)**:
1. **Encoder Trigger Missing**: No callback from `NetStream.Publish.Start` ‚Üí encoder loops
   - **Location**: `MinimalRtmpsClient.kt:293` and `RootEncoderService.kt:270-320`
   - **Impact**: YouTube disconnects after 5-7s (empty stream)
   - **Fix Required**: Add `onPublishStarted` callback, refactor encoder startup

**HIGH (Prevents long-term streaming)**:
2. **Keep-Alive Timer Missing**: No proactive PingRequest when idle
   - **Location**: `MinimalRtmpsClient.kt` (NOT IMPLEMENTED)
   - **Impact**: NAT/carrier timeout after 20-30s of no inbound traffic
   - **Fix Required**: Implement 10-15s timer with `sendPingRequest()`

3. **Extended Timestamp Not Implemented**: Streams > 4.66 hours will fail
   - **Location**: `MinimalRtmpsClient.kt:560-585` (sendCommandWithTimestamp)
   - **Impact**: Timestamp overflow after 16777215 ms
   - **Fix Required**: Detect ts >= 0xFFFFFF, use extended timestamp field

**MEDIUM (Robustness)**:
4. **Reconnection Logic Untested**: Disconnect handling exists but not tested
   - **Location**: `MinimalRtmpsClient.kt` (partially implemented)
   - **Impact**: Stream dies permanently on transient network issues
   - **Fix Required**: Test and refine reconnection with backoff

5. **Timestamp Monotonicity Not Enforced**: Could send backward timestamps
   - **Location**: `RootEncoderService.kt:530` (encodeVideoLoop)
   - **Impact**: YouTube/CDN may reject non-monotonic timestamps
   - **Fix Required**: Add lastTimestamp tracking and enforcement

---

### 4.3 Next Steps

**Immediate (Fix Critical Gap)**:
1. Add `onPublishStarted: (() -> Unit)?` callback to `MinimalRtmpsClient`
2. Invoke callback in `handleOnStatus()` when code = "NetStream.Publish.Start"
3. Refactor `RootEncoderService.startStreaming()`:
   - Setup encoders but DON'T start loops
   - Register `onPublishStarted` callback
   - Move encoding loop launch into callback
4. Add safety timer (500ms) to detect if frames are flowing
5. Test: Expect "First video frame" log within 200-300ms of publish

**Short-Term (Robustness)**:
6. Implement keep-alive timer (10-15s interval, 8s idle threshold)
7. Add timestamp monotonicity enforcement
8. Test reconnection logic with intentional disconnects

**Long-Term (Production-Ready)**:
9. Implement extended timestamp handling (32-bit)
10. Add adaptive bitrate based on network conditions
11. Add stream health monitoring (frame drops, buffer underruns)
12. Add automatic GOP/bitrate adjustment on poor network

---

## Appendix A: RTMP Constants

**File**: `RtmpConsts.kt` (referenced but not shown)

```kotlin
object RtmpConsts {
    // Protocol Control Messages (1-6)
    const val SET_CHUNK_SIZE = 1
    const val ABORT = 2
    const val ACKNOWLEDGEMENT = 3
    const val USER_CONTROL = 4
    const val WINDOW_ACK_SIZE = 5
    const val SET_PEER_BANDWIDTH = 6
    
    // Media Messages
    const val AUDIO_MESSAGE = 8
    const val VIDEO_MESSAGE = 9
    
    // Command Messages
    const val DATA_MESSAGE_AMF0 = 18
    const val COMMAND_AMF0 = 20
    const val COMMAND_AMF3 = 17
    
    // User Control Event Types
    const val USER_CTRL_STREAM_BEGIN = 0
    const val USER_CTRL_STREAM_EOF = 1
    const val USER_CTRL_STREAM_DRY = 2
    const val USER_CTRL_SET_BUFFER_LENGTH = 3
    const val USER_CTRL_STREAM_IS_RECORDED = 4
    const val USER_CTRL_PING_REQUEST = 6
    const val USER_CTRL_PING_RESPONSE = 7
}
```

---

## Appendix B: StreamConfig Reference

**File**: `StreamConfig.kt`

```kotlin
package com.screenlive.app.config

object StreamConfig {
    // RTMP Protocol
    var rtmpChunkSize: Int = 4096
    var ackWindowThreshold: Double = 0.6  // Send ACK at 60% of windowAckSize
    var maxReconnectAttempts: Int = 5
    var initialBackoffMs: Long = 500
    var maxBackoffMs: Long = 10_000
    
    // Encoder Presets
    enum class Preset(
        val width: Int,
        val height: Int,
        val fps: Int,
        val videoBitrate: Int,
        val audioBitrate: Int,
        val gopSeconds: Int,
        val bFrames: Int
    ) {
        PRESET_720P60_STABLE(
            width = 1280,
            height = 720,
            fps = 60,
            videoBitrate = 6_000_000,  // 6 Mbps (YouTube recommended for 720p60)
            audioBitrate = 160_000,     // 160 kbps (YouTube recommended)
            gopSeconds = 2,
            bFrames = 2
        ),
        PRESET_1080P60_QUALITY(
            width = 1920,
            height = 1080,
            fps = 60,
            videoBitrate = 9_000_000,  // 9 Mbps (YouTube recommended for 1080p60)
            audioBitrate = 192_000,     // 192 kbps
            gopSeconds = 2,
            bFrames = 2
        ),
        PRESET_480P30_LOW(
            width = 854,
            height = 480,
            fps = 30,
            videoBitrate = 2_500_000,  // 2.5 Mbps
            audioBitrate = 128_000,     // 128 kbps
            gopSeconds = 2,
            bFrames = 0                 // No B-frames for low latency
        )
    }
}
```

---

## Appendix C: References

**RTMP Specification**:
- [Adobe RTMP Specification 1.0](https://www.adobe.com/devnet/rtmp.html)
- [RTMP Chunk Stream](https://rtmp.veriskope.com/docs/spec/#5-chunking)
- [RTMP Message Formats](https://rtmp.veriskope.com/docs/spec/#6-rtmp-message-formats)

**FLV Specification**:
- [Adobe FLV File Format Specification v10.1](https://www.adobe.com/content/dam/acom/en/devnet/flv/video_file_format_spec_v10.pdf)

**H.264/AVC**:
- [ISO/IEC 14496-10 (AVC/H.264)](https://www.itu.int/rec/T-REC-H.264)
- [AVCDecoderConfigurationRecord](https://github.com/facebook/mp4parse-rust/blob/master/mp4parse/src/avc.rs)

**AAC Audio**:
- [ISO/IEC 14496-3 (AAC)](https://www.iso.org/standard/53943.html)
- [AudioSpecificConfig](https://wiki.multimedia.cx/index.php/MPEG-4_Audio)

**YouTube Live Requirements**:
- [YouTube Live Encoder Settings](https://support.google.com/youtube/answer/2853702)
- Minimum bitrate 720p60: 4.5-6 Mbps
- Minimum bitrate 1080p60: 6-9 Mbps
- GOP (keyframe interval): 2-4 seconds
- Audio: AAC-LC, 48 kHz, mono/stereo, 128-160 kbps

---

**Document Status**: COMPLETE (covers all three requested sections)  
**Last Updated**: 2024 (based on Window ACK fix completion)  
**Critical Finding**: Encoder trigger missing - `NetStream.Publish.Start` does not launch encoding loops  
**Priority Fix**: Implement `onPublishStarted` callback and refactor encoder startup sequence
